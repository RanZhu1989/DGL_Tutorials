{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\" \n",
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dgl():\n",
    "    \"\"\"Load citation network dataset using DGL\"\"\"\n",
    "    print('Loading Cora dataset...')\n",
    "\n",
    "    # 使用 DGL 加载 Cora 数据集\n",
    "    dataset = CoraGraphDataset()\n",
    "    graph = dataset[0]\n",
    "    N = graph.number_of_nodes()  # 节点数\n",
    "\n",
    "    # 获取节点特征和标签\n",
    "    features = graph.ndata['feat']\n",
    "    # features = normalize_features(features.numpy())  # 归一化特征\n",
    "    labels = graph.ndata['label']\n",
    "\n",
    "    # 获取训练、验证和测试集的索引\n",
    "    train_mask = graph.ndata['train_mask']\n",
    "    val_mask = graph.ndata['val_mask']\n",
    "    test_mask = graph.ndata['test_mask']\n",
    "\n",
    "    # 转换为 PyTorch 张量\n",
    "    features = torch.FloatTensor(features)\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    # 获取边属性（邻接矩阵）\n",
    "    adj = graph.adjacency_matrix().to_dense()\n",
    "    adj = torch.FloatTensor(adj.numpy())\n",
    "    #NOTE: 为图的边赋予新的三维属性\n",
    "    edge_attr = [adj, adj.t(), adj + adj.t()] # 这里人为构建一个3维的边属性 [3 * N * N]\n",
    "    P = len(edge_attr)  # 边属性的维度\n",
    "    edge_attr = torch.stack(edge_attr, dim=0)\n",
    "    edge_attr = DSN(edge_attr) # 双随机归一化\n",
    "    edge_attr_reshaped = edge_attr[:, graph.edges()[0], graph.edges()[1]]\n",
    "    graph.edata['feat'] = edge_attr_reshaped.t()\n",
    "\n",
    "    return graph, edge_attr, features, labels, train_mask, val_mask, test_mask\n",
    "\n",
    "def DSN2(t):\n",
    "    a=t.sum(dim=1,keepdim=True)\n",
    "    b=t.sum(dim=0,keepdim=True)\n",
    "    lamb=torch.cat([a.squeeze(),b.squeeze()],dim=0).max()\n",
    "    r=t.shape[0]*lamb-t.sum(dim=0).sum(dim=0)\n",
    "    \n",
    "    a=a.expand(-1,t.shape[1])\n",
    "    b=b.expand(t.shape[0],-1)\n",
    "    tt=t+(lamb**2-lamb*(a+b)+a*b)/r\n",
    "\n",
    "    ttmatrix=tt/tt.sum(dim=0)[0]\n",
    "    ttmatrix=torch.where(t>0,ttmatrix,t)\n",
    "    return ttmatrix\n",
    "\n",
    "\n",
    "def DSN(x):\n",
    "    \"\"\"Doubly stochastic normalization\"\"\"\n",
    "    p=x.shape[0]\n",
    "    y1=[]\n",
    "    for i in range(p):\n",
    "        y1.append(DSN2(x[i]))\n",
    "    y1=torch.stack(y1,dim=0)\n",
    "    return y1\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"input is a numpy array\"\"\" \n",
    "    rowsum = mx.sum(axis=1)\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = np.diag(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, edge_attr, features, labels, train_mask, val_mask, test_mask = load_data_dgl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Node features\")\n",
    "print(g.ndata)\n",
    "print(\"Edge features\")\n",
    "print(g.edata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X^l=\\sigma\\left[\\|_{p=1}^P\\left(\\alpha_{. \\cdot p}^l\\left(X^{l-1}, E_{\\cdot \\cdot p}^{l-1}\\right) g^l\\left(X^{l-1}\\right)\\right)\\right] .\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "g^l\\left(X^{l-1}\\right)=W^l X^{l-1},\\\\\n",
    "f^l\\left(X_{i \\cdot}^{l-1}, X_{j .}^{l-1}\\right)=\\exp \\left\\{\\mathrm{L}\\left(a^T\\left[W X_{i \\cdot}^{l-1} \\| W X_{j .}^{l-1}\\right]\\right)\\right\\}\\\\\n",
    "\\alpha_{\\cdot \\cdot p}^l=\\operatorname{DS}\\left(\\hat{\\alpha}_{\\cdot \\cdot p}^l\\right),\\\\\n",
    "\\hat{\\alpha}_{i j p}^l=f^l\\left(X_{i .}^{l-1}, X_j^{l-1}\\right) E_{i j p}^{l-1},\\\\\n",
    "E^l=\\alpha^l\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGNNA_Conv(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim_in:int, # 输入h的特征维度\n",
    "                 dim_h:int, # 各节点h的l+1层特征维度\n",
    "                 dropout:float,\n",
    "                 node_att_agger: bool = False # 最后节点特征是N*(F*P) 还是聚合为 N*F\n",
    "                 ):\n",
    "        super(EGNNA_Conv, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.FC1 = nn.Linear(dim_in, dim_h) # W\n",
    "        self.FC2 = nn.Linear(dim_h,1) # a1\n",
    "        self.FC3 = nn.Linear(dim_h,1) # a2\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2) #NOTE: negative_slope=0.2\n",
    "        self.node_att_agger = node_att_agger\n",
    "    \n",
    "    def forward(self, h, e):\n",
    "        '''\n",
    "        INPUT:\n",
    "        g: graph,\n",
    "        h: node_feature  shape = [N * dim_node_features]\n",
    "        e: edge_feature shape = [dim_edge_features * N * N]\n",
    "        \n",
    "        OUTPUT:\n",
    "        new_h : new node feature shape = [N * (dim_node_features*dim_edge_features)]\n",
    "        alpha: new edge feature shape = [dim_edge_features * N * N]\n",
    "        \n",
    "        '''\n",
    "        # 用FC1先算W @ h\n",
    "        Wh = self.FC1(h) # N * dim_out_features\n",
    "        # 计算注意力系数(N * N )\n",
    "        fXX = self.leakyrelu(self.FC2(Wh) + self.FC3(Wh).t()) # N * N\n",
    "        alpha = fXX * e # 广播点乘 [N * N]  .*  [P * N * N] .= [P * N * N]\n",
    "        alpha = torch.where(e>0, alpha, -9e15*torch.ones_like(alpha)) # 将不存在的边的注意力系数设置为0\n",
    "        alpha = F.softmax(alpha, dim=1) # 对每个节点的所有邻居进行softmax\n",
    "        \n",
    "        #NOTE: alpha := new edge feature (e)\n",
    "        \n",
    "        attention = F.dropout(alpha, self.dropout, training=self.training) # dropout\n",
    "        \n",
    "        new_h = torch.empty((e.shape[0], Wh.shape[0], Wh.shape[1]), device=Wh.device)\n",
    "        for i in range(e.shape[0]):\n",
    "            new_h[i] = torch.matmul(attention[i], Wh)\n",
    "        \n",
    "        if not self.node_att_agger:\n",
    "            new_h = new_h.permute(1,2,0).reshape(new_h.shape[1], -1) # N * (P * F)\n",
    "            \n",
    "            return F.elu(new_h), alpha\n",
    "        \n",
    "        else:\n",
    "            new_h = torch.sum(new_h, dim=0) # N * F\n",
    "            \n",
    "            return new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead_EGNNA_Classifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_nfeat:int, \n",
    "                 dim_efeat:int, # P\n",
    "                 dim_hidden:int, # for [input_layer, hidden_layer]\n",
    "                 dim_out:int,# for output_layer\n",
    "                 dropout:float,\n",
    "                 n_heads:int # for [input_layer, hidden_layer]\n",
    "                 ):\n",
    "        super(MultiHead_EGNNA_Classifier, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        # 1. input layer\n",
    "        self.attentions = [EGNNA_Conv(dim_nfeat, dim_hidden[0], dropout) for _ in range(n_heads[0])]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "            \n",
    "        # 2. Hidden layer\n",
    "        self.hidden_atts = [EGNNA_Conv(n_heads[0]*dim_efeat*dim_hidden[0],\n",
    "                                       dim_hidden[1], dropout) for _ in range(n_heads[1])]\n",
    "        for i, hidden_att in enumerate(self.hidden_atts):\n",
    "            self.add_module('hidden_att_{}'.format(i), hidden_att)\n",
    "        \n",
    "        # 3. Output layer\n",
    "        self.out_att = EGNNA_Conv(n_heads[0]*dim_efeat*dim_hidden[0], dim_out, dropout, node_att_agger=True)\n",
    "        \n",
    "    def forward(self,  h, e):\n",
    "        \n",
    "        # Input layer\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        temp_h = []\n",
    "        for att in self.attentions:\n",
    "            h_, e = att( h, e)\n",
    "            temp_h.append(h_)\n",
    "        h = torch.cat(temp_h, dim=1)\n",
    "            \n",
    "        # Hidden layer\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        temp_h = []\n",
    "        for att in self.hidden_atts:\n",
    "            h_, e = att( h, e)\n",
    "            temp_h.append(h_)\n",
    "        h = torch.cat(temp_h, dim=1)\n",
    "        \n",
    "        # Output layer\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        h = F.elu(self.out_att( h, e)) # 输出各分类的得分情况\n",
    "        \n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiHead_EGNNA_Classifier(dim_nfeat=features.shape[1],\n",
    "                                   dim_efeat=edge_attr.shape[0],\n",
    "                                   dim_hidden=[64,8],\n",
    "                                   dim_out=int(labels.max()) + 1,\n",
    "                                   dropout=0.5,\n",
    "                                   n_heads=[1,8]\n",
    ")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    edge_attr = edge_attr.cuda()\n",
    "    labels = labels.cuda()\n",
    "    train_mask = train_mask.cuda()\n",
    "    val_mask = val_mask.cuda()\n",
    "    test_mask = test_mask.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "for e in range(100):\n",
    "    # set model to training mode  \n",
    "    model.train()\n",
    "    logits = model(features, edge_attr)\n",
    "    pred = logits.argmax(1) # 返回得分最大的索引\n",
    "    train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "    val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    logits = model(features, edge_attr)\n",
    "    pred = logits.argmax(1)\n",
    "    test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "    \n",
    "\n",
    "    if best_val_acc < val_acc:\n",
    "          best_val_acc = val_acc\n",
    "          best_test_acc = test_acc\n",
    "          pass\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "            print(\n",
    "                f\"In epoch {e}, loss: {loss:.3f}, val acc: {val_acc:.3f} (best {best_val_acc:.3f}), test acc: {test_acc:.3f} (best {best_test_acc:.3f})\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

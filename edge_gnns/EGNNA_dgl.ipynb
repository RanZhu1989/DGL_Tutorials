{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\" \n",
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dgl():\n",
    "    \"\"\"Load citation network dataset using DGL\"\"\"\n",
    "    print('Loading Cora dataset...')\n",
    "\n",
    "    # 使用 DGL 加载 Cora 数据集\n",
    "    dataset = CoraGraphDataset()\n",
    "    graph = dataset[0]\n",
    "    N = graph.number_of_nodes()  # 节点数\n",
    "\n",
    "    # 获取节点特征和标签\n",
    "    features = graph.ndata['feat']\n",
    "    labels = graph.ndata['label']\n",
    "\n",
    "    # 获取训练、验证和测试集的索引\n",
    "    train_mask = graph.ndata['train_mask']\n",
    "    val_mask = graph.ndata['val_mask']\n",
    "    test_mask = graph.ndata['test_mask']\n",
    "\n",
    "    # 转换为 PyTorch 张量\n",
    "    features = torch.FloatTensor(features)\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    # 获取边属性（邻接矩阵）\n",
    "    adj = graph.adjacency_matrix().to_dense()\n",
    "    adj = torch.FloatTensor(adj.numpy())\n",
    "    #NOTE: 为图的边赋予新的三维属性\n",
    "    edge_attr = [adj, adj.t(), adj + adj.t()] # 这里人为构建一个3维的边属性 [3 * N * N]\n",
    "    P = len(edge_attr)  # 边属性的维度\n",
    "    edge_attr = torch.stack(edge_attr, dim=0)\n",
    "    edge_attr = DSN(edge_attr) # 双随机归一化\n",
    "    edge_attr_reshaped = edge_attr[:, graph.edges()[0], graph.edges()[1]]\n",
    "    graph.edata['feat'] = edge_attr_reshaped.t()\n",
    "\n",
    "    return graph, edge_attr, features, labels, train_mask, val_mask, test_mask\n",
    "\n",
    "def DSN2(t):\n",
    "    a=t.sum(dim=1,keepdim=True)\n",
    "    b=t.sum(dim=0,keepdim=True)\n",
    "    lamb=torch.cat([a.squeeze(),b.squeeze()],dim=0).max()\n",
    "    r=t.shape[0]*lamb-t.sum(dim=0).sum(dim=0)\n",
    "    \n",
    "    a=a.expand(-1,t.shape[1])\n",
    "    b=b.expand(t.shape[0],-1)\n",
    "    tt=t+(lamb**2-lamb*(a+b)+a*b)/r\n",
    "\n",
    "    ttmatrix=tt/tt.sum(dim=0)[0]\n",
    "    ttmatrix=torch.where(t>0,ttmatrix,t)\n",
    "    return ttmatrix\n",
    "\n",
    "\n",
    "def DSN(x):\n",
    "    \"\"\"Doubly stochastic normalization\"\"\"\n",
    "    p=x.shape[0]\n",
    "    y1=[]\n",
    "    for i in range(p):\n",
    "        y1.append(DSN2(x[i]))\n",
    "    y1=torch.stack(y1,dim=0)\n",
    "    return y1\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    \"\"\"input is a numpy array\"\"\" \n",
    "    rowsum = mx.sum(axis=1)\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = np.diag(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Cora dataset...\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "g, edge_attr, features, labels, idx_train, idx_val, idx_test = load_data_dgl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features\n",
      "{'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "Edge features\n",
      "{'feat': tensor([[0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0062, 0.0062, 0.0062],\n",
      "        [0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Node features\")\n",
    "print(g.ndata)\n",
    "print(\"Edge features\")\n",
    "print(g.edata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X^l=\\sigma\\left[\\|_{p=1}^P\\left(\\alpha_{. \\cdot p}^l\\left(X^{l-1}, E_{\\cdot \\cdot p}^{l-1}\\right) g^l\\left(X^{l-1}\\right)\\right)\\right] .\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "g^l\\left(X^{l-1}\\right)=W^l X^{l-1},\\\\\n",
    "f^l\\left(X_{i \\cdot}^{l-1}, X_{j .}^{l-1}\\right)=\\exp \\left\\{\\mathrm{L}\\left(a^T\\left[W X_{i \\cdot}^{l-1} \\| W X_{j .}^{l-1}\\right]\\right)\\right\\}\\\\\n",
    "\\alpha_{\\cdot \\cdot p}^l=\\operatorname{DS}\\left(\\hat{\\alpha}_{\\cdot \\cdot p}^l\\right),\\\\\n",
    "\\hat{\\alpha}_{i j p}^l=f^l\\left(X_{i .}^{l-1}, X_j^{l-1}\\right) E_{i j p}^{l-1},\\\\\n",
    "E^l=\\alpha^l\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGNNA_Conv(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim_in:int, # 输入h的特征维度\n",
    "                 dim_h:int, # 各节点h的l+1层特征维度\n",
    "                 dropout:float,\n",
    "                 node_att_agger: bool = False # 最后节点特征是N*(F*P) 还是聚合为 N*F\n",
    "                 ):\n",
    "        super(EGNNA_Conv, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.FC1 = nn.Linear(dim_in, dim_h) # W\n",
    "        self.FC2 = nn.Linear(dim_h,1) # a1\n",
    "        self.FC3 = nn.Linear(dim_h,1) # a2\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2) #NOTE: negative_slope=0.2\n",
    "        self.node_att_agger = node_att_agger\n",
    "    \n",
    "    def forward(self, g:dgl.graph, h, e):\n",
    "        '''\n",
    "        INPUT:\n",
    "        g: graph,\n",
    "        h: node_feature  shape = [N * dim_node_features]\n",
    "        e: edge_feature shape = [dim_edge_features * N * N]\n",
    "        \n",
    "        OUTPUT:\n",
    "        new_h : new node feature shape = [N * (dim_node_features*dim_edge_features)]\n",
    "        alpha: new edge feature shape = [dim_edge_features * N * N]\n",
    "        \n",
    "        '''\n",
    "        # 先算W dot h\n",
    "        Wh = self.FC1(h) # N * dim_out_features\n",
    "        # 计算注意力系数(N * N )\n",
    "        fXX = self.leakyrelu(self.FC2(Wh) + self.FC3(Wh).t()) # N * N\n",
    "        alpha = fXX * e # 广播点乘 [N * N]  .*  [P * N * N] .= [P * N * N]\n",
    "        alpha = torch.where(e>0, alpha, -9e15*torch.ones_like(alpha)) # 将不存在的边的注意力系数设置为0\n",
    "        alpha = F.softmax(alpha, dim=1) # 对每个节点的所有邻居进行softmax\n",
    "        \n",
    "        #NOTE: alpha := new edge feature (e)\n",
    "        \n",
    "        attention = F.dropout(alpha, self.dropout, training=self.training) # dropout\n",
    "        attention_shaped = attention[:, g.edges()[0], g.edges()[1]] # 将注意力系数转换为边的形状\n",
    "        \n",
    "        with g.local_scope():\n",
    "            g.ndata['Wh'] = Wh\n",
    "            g.edata['alpha'] = attention_shaped.t()\n",
    "            '''\n",
    "            对于每个节点: \n",
    "            --------准备------\n",
    "            1. 获取P个通道上的注意力系数 alpha P\n",
    "            2. 获取经过FC1线性变换后的特征 wh\n",
    "            --------消息传递------\n",
    "            3. 源节点u的Wh特征与各通道(P个)边注意力系数alpha相乘，存入目标节点v mailbox的'msg'特征\n",
    "            --------聚合------\n",
    "            4. 各节点mailbox的'msg'特征把P个通道拼起来，得到新的节点特征h\n",
    "            '''\n",
    "            if not self.node_att_agger:\n",
    "                g.update_all(\n",
    "                    message_func = fn.e_mul_u('Wh', 'alpha', 'msg'), \n",
    "                    reduce_func = cat_channels,\n",
    "                )\n",
    "                \n",
    "                new_h = F.elu(g.ndata['h'])\n",
    "                \n",
    "                return new_h, alpha\n",
    "            \n",
    "            else:\n",
    "                g.update_all(\n",
    "                        message_func = fn.e_mul_u('Wh', 'alpha', 'msg'), \n",
    "                        reduce_func = cat_channels,\n",
    "                    )\n",
    "                \n",
    "                new_h = g.ndata['h']\n",
    "                \n",
    "                return new_h\n",
    "\n",
    "'''\n",
    "需要自定义一个函数，将P个通道的消息拼接起来\n",
    "'''\n",
    "def cat_channels(nodes):\n",
    "    msgs = nodes.mailbox['msg'] # shape = [N * P * F]\n",
    "    agg_msg = msgs.view(msgs.shape[0], -1)  # 形状为 (num_nodes, P*F)\n",
    "    return {'h': agg_msg}\n",
    "\n",
    "def agger_channels(nodes):\n",
    "    msgs = nodes.mailbox['msg']\n",
    "    agg_msg = msgs.sum(dim=1)  # 形状为 (num_nodes, F)\n",
    "    return {'h': agg_msg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead_EGNNA_Classifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_nfeat:int, # F\n",
    "                 dim_efeat:int, # P\n",
    "                 dim_hidden:int, # for [input_layer, hidden_layer]\n",
    "                 dim_out:int,# for output_layer\n",
    "                 dropout:float,\n",
    "                 n_heads:int # for [input_layer, hidden_layer]\n",
    "                 ):\n",
    "        super(MultiHead_EGNNA_Classifier, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        # 1. input layer\n",
    "        self.attentions = [EGNNA_Conv(dim_nfeat, dim_hidden[0], dropout) for _ in range(n_heads[0])]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "            \n",
    "        # 2. Hidden layer\n",
    "        self.hidden_atts = [EGNNA_Conv(n_heads[0]*dim_efeat*dim_hidden[0],\n",
    "                                       dim_hidden[1], dropout) for _ in range(n_heads[0])]\n",
    "        for i, hidden_att in enumerate(self.hidden_atts):\n",
    "            self.add_module('hidden_att_{}'.format(i), hidden_att)\n",
    "        \n",
    "        # 3. Output layer\n",
    "        self.out_att = EGNNA_Conv(n_heads[0]*dim_efeat*dim_hidden[0], dim_out, dropout, node_att_agger=True)\n",
    "        \n",
    "    def forward(self, g, h, e):\n",
    "        \n",
    "        # Input layer\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        temp_h = []\n",
    "        for att in self.attentions:\n",
    "            h_, e = att(g, h, e)\n",
    "            temp_h.append(h_)\n",
    "        h = torch.cat(temp_h, dim=1)\n",
    "            \n",
    "        # Hidden layer\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        temp_h = []\n",
    "        for att in self.hidden_atts:\n",
    "            h_, e = att(g, h, e)\n",
    "            temp_h.append(h_)\n",
    "        h = torch.cat(temp_h, dim=1)\n",
    "        \n",
    "        # Output layer\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        h = F.elu(self.out_att(g, h, e)) # 输出各分类的得分情况\n",
    "        \n",
    "        return h\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
